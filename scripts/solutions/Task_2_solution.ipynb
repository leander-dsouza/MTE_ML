{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Task 1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2"
      ],
      "metadata": {
        "id": "frank-statistics"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "import numpy as np\n",
        "import re"
      ],
      "outputs": [],
      "metadata": {
        "id": "hundred-making"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "sample_listofreview=['Food was damn good!:)','good food. recommended','YOU PEOPLE ARE THE BEST!!!','It tasted very bad..... too bad service as well']\n",
        "print(sample_listofreview)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Food was damn good!:)', 'good food. recommended', 'YOU PEOPLE ARE THE BEST!!!', 'It tasted very bad..... too bad service as well']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orange-restriction",
        "outputId": "269dea67-3c36-4677-8613-55416a830ce9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "sample=sample_listofreview[2]\n",
        "sample=re.sub('[!@#$%.^&*()_+}{\":?><\"}]','',sample)\n",
        "print(sample)\n",
        "sample=sample.lower()\n",
        "print('\\n')\n",
        "print(sample)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOU PEOPLE ARE THE BEST\n",
            "\n",
            "\n",
            "you people are the best\n"
          ]
        }
      ],
      "metadata": {
        "id": "refined-diving"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing the data"
      ],
      "metadata": {
        "id": "appointed-consciousness"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "processed_reviews=[]\n",
        "for review in sample_listofreview:\n",
        "    for words in review:\n",
        "        review=re.sub('[!@#$%^&*.()_+}{\":?><\"}]','',review)\n",
        "        review=review.lower()\n",
        "    processed_reviews.append(review)\n",
        "    \n",
        "print(processed_reviews)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['food was damn good', 'good food recommended', 'you people are the best', 'it tasted very bad too bad service as well']\n"
          ]
        }
      ],
      "metadata": {
        "id": "former-fifteen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collecting the uniques words"
      ],
      "metadata": {
        "id": "bored-match"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "unique_words=[]\n",
        "for review in processed_reviews:\n",
        "    for word in review.split():\n",
        "        if word not in unique_words:\n",
        "            unique_words.append(word)\n",
        "\n",
        "print('list of unique words',unique_words)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list of unique words ['food', 'was', 'damn', 'good', 'recommended', 'you', 'people', 'are', 'the', 'best', 'it', 'tasted', 'very', 'bad', 'too', 'service', 'as', 'well']\n"
          ]
        }
      ],
      "metadata": {
        "id": "quarterly-bride"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "feature_matrix=np.zeros((len(processed_reviews),len(unique_words)))\n",
        "for n,review in enumerate(processed_reviews):\n",
        "    for word in review.split():\n",
        "        feature_matrix[n][unique_words.index(word)]=review.count(word)\n",
        "        \n",
        "feature_matrix"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.],\n",
              "       [1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 2., 1., 1.,\n",
              "        2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {
        "id": "lovely-being"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assignment task\n",
        "<pre>\n",
        "Download data from <a href='https://tinyurl.com/47rx5cce'>link</a>\n",
        "\n",
        "1. process the data \n",
        "1a. remove special characters\n",
        "1b. remove non english ~words~ alphabets\n",
        "1c. remove numerical values\n",
        "1d. all words in lower case\n",
        "1e. remove punctuation\n",
        "1f. Featurize using Bag of Words technique\n",
        "\n",
        "2. Fit the Naive Bayes Classifier\n",
        "3. Report the Accuracy\n",
        "4. Compare the Results from other Variants of NB Models \n",
        "5. Write your own observations\n",
        "\n",
        "</pre>\n"
      ],
      "metadata": {
        "id": "involved-integrity"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "df = pd.read_csv('../../datasets/Tweets.csv') \n",
        "\n",
        "df['text']\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                      @VirginAmerica What @dhepburn said.\n",
              "1        @VirginAmerica plus you've added commercials t...\n",
              "2        @VirginAmerica I didn't today... Must mean I n...\n",
              "3        @VirginAmerica it's really aggressive to blast...\n",
              "4        @VirginAmerica and it's a really big bad thing...\n",
              "                               ...                        \n",
              "14635    @AmericanAir thank you we got on a different f...\n",
              "14636    @AmericanAir leaving over 20 minutes Late Flig...\n",
              "14637    @AmericanAir Please bring American Airlines to...\n",
              "14638    @AmericanAir you have my money, you change my ...\n",
              "14639    @AmericanAir we have 8 ppl so we need 2 know h...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "df['airline_sentiment'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    9178\n",
              "neutral     3099\n",
              "positive    2363\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "print(df.iloc[:,-3].value_counts(normalize = True))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative    0.626913\n",
            "neutral     0.211680\n",
            "positive    0.161407\n",
            "Name: airline_sentiment, dtype: float64\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Remove Special Characters"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "df['text'].replace(regex=True, inplace=True, to_replace=r'[@_!#$%^&*()<>?/\\|}{~:]', value=r'')\n",
        "\n",
        "df['text']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                        VirginAmerica What dhepburn said.\n",
              "1        VirginAmerica plus you've added commercials to...\n",
              "2        VirginAmerica I didn't today... Must mean I ne...\n",
              "3        VirginAmerica it's really aggressive to blast ...\n",
              "4        VirginAmerica and it's a really big bad thing ...\n",
              "                               ...                        \n",
              "14635    AmericanAir thank you we got on a different fl...\n",
              "14636    AmericanAir leaving over 20 minutes Late Fligh...\n",
              "14637    AmericanAir Please bring American Airlines to ...\n",
              "14638    AmericanAir you have my money, you change my f...\n",
              "14639    AmericanAir we have 8 ppl so we need 2 know ho...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Remove Non English Alphabets"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "df['text'].replace(regex=True, inplace=True, to_replace=r'[^A-Za-z0-9 ]+', value=r'')\n",
        "\n",
        "df['text']\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         VirginAmerica What dhepburn said\n",
              "1        VirginAmerica plus youve added commercials to ...\n",
              "2        VirginAmerica I didnt today Must mean I need t...\n",
              "3        VirginAmerica its really aggressive to blast o...\n",
              "4        VirginAmerica and its a really big bad thing a...\n",
              "                               ...                        \n",
              "14635    AmericanAir thank you we got on a different fl...\n",
              "14636    AmericanAir leaving over 20 minutes Late Fligh...\n",
              "14637    AmericanAir Please bring American Airlines to ...\n",
              "14638    AmericanAir you have my money you change my fl...\n",
              "14639    AmericanAir we have 8 ppl so we need 2 know ho...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c. Remove Numerical Values"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "# Regex to remove all the numbers and ending space to match readability\n",
        "df['text'].replace(regex=True, inplace=True, to_replace=r'\\d+\\s*', value=r'')\n",
        "\n",
        "df['text']\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         VirginAmerica What dhepburn said\n",
              "1        VirginAmerica plus youve added commercials to ...\n",
              "2        VirginAmerica I didnt today Must mean I need t...\n",
              "3        VirginAmerica its really aggressive to blast o...\n",
              "4        VirginAmerica and its a really big bad thing a...\n",
              "                               ...                        \n",
              "14635    AmericanAir thank you we got on a different fl...\n",
              "14636    AmericanAir leaving over minutes Late Flight N...\n",
              "14637    AmericanAir Please bring American Airlines to ...\n",
              "14638    AmericanAir you have my money you change my fl...\n",
              "14639    AmericanAir we have ppl so we need know how ma...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d. All words in lower case"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "df[\"text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
        "\n",
        "df[\"text\"]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         virginamerica what dhepburn said\n",
              "1        virginamerica plus youve added commercials to ...\n",
              "2        virginamerica i didnt today must mean i need t...\n",
              "3        virginamerica its really aggressive to blast o...\n",
              "4        virginamerica and its a really big bad thing a...\n",
              "                               ...                        \n",
              "14635    americanair thank you we got on a different fl...\n",
              "14636    americanair leaving over minutes late flight n...\n",
              "14637    americanair please bring american airlines to ...\n",
              "14638    americanair you have my money you change my fl...\n",
              "14639    americanair we have ppl so we need know how ma...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1e. Remove Punctuation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "df['text'].replace(regex=True, inplace=True, to_replace=r'[%s]' % string.punctuation, value=r'')\n",
        "\n",
        "df['text']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         virginamerica what dhepburn said\n",
              "1        virginamerica plus youve added commercials to ...\n",
              "2        virginamerica i didnt today must mean i need t...\n",
              "3        virginamerica its really aggressive to blast o...\n",
              "4        virginamerica and its a really big bad thing a...\n",
              "                               ...                        \n",
              "14635    americanair thank you we got on a different fl...\n",
              "14636    americanair leaving over minutes late flight n...\n",
              "14637    americanair please bring american airlines to ...\n",
              "14638    americanair you have my money you change my fl...\n",
              "14639    americanair we have ppl so we need know how ma...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1f. Featurize using Bag of Words technique"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "processed_tweets = df['text']\n",
        "\n",
        "unique_words=[]\n",
        "for tweet in processed_tweets:\n",
        "    for word in tweet.split():\n",
        "        if word not in unique_words:\n",
        "            unique_words.append(word)\n",
        "\n",
        "\n",
        "feature_matrix = np.zeros((len(processed_tweets),len(unique_words)))\n",
        "for n,review in enumerate(processed_tweets):\n",
        "    for word in review.split():\n",
        "        feature_matrix[n][unique_words.index(word)]=review.count(word)\n",
        "        \n",
        "feature_matrix\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fit the Naive Bayes Classifier"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a. Mapping the airline_sentiment data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "# Mapping values from -1 to 1\n",
        "\n",
        "df['airline_sentiment'] = df['airline_sentiment'].map({'negative': -1, 'neutral': 0, 'positive': 1})\n",
        "df['airline_sentiment'] = df['airline_sentiment'].astype('category')\n",
        "df.head(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_created</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.703060e+17</td>\n",
              "      <td>0</td>\n",
              "      <td>virginamerica what dhepburn said</td>\n",
              "      <td>24-02-2015 11:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>1</td>\n",
              "      <td>virginamerica plus youve added commercials to ...</td>\n",
              "      <td>24-02-2015 11:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>0</td>\n",
              "      <td>virginamerica i didnt today must mean i need t...</td>\n",
              "      <td>24-02-2015 11:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>-1</td>\n",
              "      <td>virginamerica its really aggressive to blast o...</td>\n",
              "      <td>24-02-2015 11:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>-1</td>\n",
              "      <td>virginamerica and its a really big bad thing a...</td>\n",
              "      <td>24-02-2015 11:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>-1</td>\n",
              "      <td>virginamerica seriously would pay a flight for...</td>\n",
              "      <td>24-02-2015 11:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>1</td>\n",
              "      <td>virginamerica yes nearly every time i fly vx t...</td>\n",
              "      <td>24-02-2015 11:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.703000e+17</td>\n",
              "      <td>0</td>\n",
              "      <td>virginamerica really missed a prime opportunit...</td>\n",
              "      <td>24-02-2015 11:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5.703000e+17</td>\n",
              "      <td>1</td>\n",
              "      <td>virginamerica well i didntbut now i do d</td>\n",
              "      <td>24-02-2015 11:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5.702950e+17</td>\n",
              "      <td>1</td>\n",
              "      <td>virginamerica it was amazing and arrived an ho...</td>\n",
              "      <td>24-02-2015 10:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       tweet_id airline_sentiment  \\\n",
              "0  5.703060e+17                 0   \n",
              "1  5.703010e+17                 1   \n",
              "2  5.703010e+17                 0   \n",
              "3  5.703010e+17                -1   \n",
              "4  5.703010e+17                -1   \n",
              "5  5.703010e+17                -1   \n",
              "6  5.703010e+17                 1   \n",
              "7  5.703000e+17                 0   \n",
              "8  5.703000e+17                 1   \n",
              "9  5.702950e+17                 1   \n",
              "\n",
              "                                                text     tweet_created  \n",
              "0                   virginamerica what dhepburn said  24-02-2015 11:35  \n",
              "1  virginamerica plus youve added commercials to ...  24-02-2015 11:15  \n",
              "2  virginamerica i didnt today must mean i need t...  24-02-2015 11:15  \n",
              "3  virginamerica its really aggressive to blast o...  24-02-2015 11:15  \n",
              "4  virginamerica and its a really big bad thing a...  24-02-2015 11:14  \n",
              "5  virginamerica seriously would pay a flight for...  24-02-2015 11:14  \n",
              "6  virginamerica yes nearly every time i fly vx t...  24-02-2015 11:13  \n",
              "7  virginamerica really missed a prime opportunit...  24-02-2015 11:12  \n",
              "8           virginamerica well i didntbut now i do d  24-02-2015 11:11  \n",
              "9  virginamerica it was amazing and arrived an ho...  24-02-2015 10:53  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2b. Splitting the data into training and test sets and fitting the model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = feature_matrix\n",
        "y = df['airline_sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
        "\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Report the Accuracy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "nb_y_pred = nb.predict(X_test)\n",
        "\n",
        "print(\"Accuracy of the NB model is: \", accuracy_score(y_test, nb_y_pred))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the NB model is:  0.7565573770491804\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Compare the results from variants of other NB models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gb = GaussianNB()\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "gb.score(X_test, y_test)\n",
        "\n",
        "gb_y_pred = gb.predict(X_test)\n",
        "\n",
        "print(\"Accuracy of the GaussianNB model is: \", accuracy_score(y_test, gb_y_pred))\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the GaussianNB model is:  0.4931693989071038\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Write your own observations"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The classes have been identified using `value_counts()` method, and their distribution through `normalize` method.\n",
        "\n",
        "* We move ahead with Multinomial Naive Bayes model instead of Bernoulli Naive Bayes model, as the data is not binary.\n",
        "\n",
        "* Regex has been used for preprocessing the data for the preparation of the feature matrix.\n",
        "\n",
        "* The feature matrix has been constructed using the BoW Technique.\n",
        "\n",
        "* The classes have been mapped from `positive`, `neutral` and `negative` to `1`, `0` and `-1` respectively.\n",
        "\n",
        "* The training and test data have been split into training and test sets using the `train_test_split` method.(ratio 3:1)\n",
        "\n",
        "* The model has been fit using Multinomial NB, and the accuracy is reported as `0.756`\n",
        "\n",
        "* The model has been also fit by using Gaussian NB, and the accuracy is reported as `0.493`.\n",
        "Thus the dataset doesn't follow a normal distribution, as the accuracy of the model is not as good as Multinomial NB.\n"
      ],
      "metadata": {}
    }
  ]
}